{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load datasets\n",
        "train_features = pd.read_csv(\"DengAI_train.csv\")\n",
        "train_labels = pd.read_csv(\"DengAI_Predicting_Disease_Spread_-_Training_Data_Labels.csv\")\n",
        "test_features = pd.read_csv(\"DengAI_test.csv\")\n",
        "\n",
        "# Merge train features and labels\n",
        "train_df = pd.merge(train_features, train_labels, on=[\"city\", \"year\", \"weekofyear\"])\n",
        "\n",
        "# Forward fill missing values (time-series)\n",
        "train_df.fillna(method='ffill', inplace=True)\n",
        "test_features.fillna(method='ffill', inplace=True)\n",
        "\n",
        "# Drop 'week_start_date' since it's non-numeric\n",
        "train_df.drop(columns=['week_start_date'], inplace=True)\n",
        "test_features.drop(columns=['week_start_date'], inplace=True)\n",
        "\n",
        "# Normalize numeric features\n",
        "scaler = StandardScaler()\n",
        "X_all = pd.concat([train_df.drop(columns=['total_cases']), test_features], axis=0)\n",
        "X_scaled = scaler.fit_transform(X_all.select_dtypes(include=[np.number]))\n",
        "\n",
        "# Restore DataFrame with scaled values\n",
        "X_all_scaled = pd.DataFrame(X_scaled, columns=X_all.select_dtypes(include=[np.number]).columns)\n",
        "X_all_scaled[['city', 'year', 'weekofyear']] = X_all[['city', 'year', 'weekofyear']].reset_index(drop=True)\n",
        "\n",
        "# Re-split\n",
        "X_train = X_all_scaled.iloc[:len(train_df)]\n",
        "X_test = X_all_scaled.iloc[len(train_df):]\n",
        "y_train = train_df['total_cases']\n"
      ],
      "metadata": {
        "id": "59XW8Wmgu9nQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fa192d2-71c4-42e1-eb5e-70dc8aa84e3a"
      },
      "id": "59XW8Wmgu9nQ",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-29-7fa4144b01b5>:14: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  train_df.fillna(method='ffill', inplace=True)\n",
            "<ipython-input-29-7fa4144b01b5>:15: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  test_features.fillna(method='ffill', inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ğŸ§  ä¼˜åŒ–ç‰ˆ GRU æ¨¡å‹ï¼šåˆ†åˆ«ä¸º SJ å’Œ IQ å»ºæ¨¡ï¼Œç›®æ ‡é™ä½ RMSE å’Œ MAE\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, GRU, Dense, Dropout, LayerNormalization\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow as tf\n",
        "\n",
        "def run_gru_model(city_code, window):\n",
        "    print(f\"\\nğŸ™ï¸ Running GRU for city: {city_code.upper()} (window={window})\")\n",
        "\n",
        "    # Step 1: Load and merge data\n",
        "    features_df = pd.read_csv('DengAI_train.csv')\n",
        "    labels_df = pd.read_csv('DengAI_Predicting_Disease_Spread_-_Training_Data_Labels.csv')\n",
        "    df = pd.merge(features_df, labels_df, on=['city', 'year', 'weekofyear'])\n",
        "\n",
        "    # Step 2: Filter city and clean\n",
        "    df = df[df['city'] == city_code].copy()\n",
        "    df.reset_index(drop=True, inplace=True)\n",
        "    df.drop(columns=['week_start_date', 'city'], inplace=True)\n",
        "    df = df.ffill()\n",
        "\n",
        "    # Step 3: Scale features\n",
        "    scaler = StandardScaler()\n",
        "    X = df.drop(columns=['total_cases'])\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "    y = np.log1p(df['total_cases'])  # log transform\n",
        "\n",
        "    # Step 4: Create sequences\n",
        "    def create_sequences(X, y, window):\n",
        "        Xs, ys = [], []\n",
        "        for i in range(window, len(X)):\n",
        "            Xs.append(X[i-window:i])\n",
        "            ys.append(y[i])\n",
        "        return np.array(Xs), np.array(ys)\n",
        "\n",
        "    X_seq, y_seq = create_sequences(X_scaled, y, window=window)\n",
        "    split = int(0.8 * len(X_seq))\n",
        "    X_train, X_test = X_seq[:split], X_seq[split:]\n",
        "    y_train, y_test = y_seq[:split], y_seq[split:]\n",
        "\n",
        "    if len(X_train) == 0 or len(X_test) == 0:\n",
        "        print(\"âŒ Not enough data to train GRU for this city.\")\n",
        "        return\n",
        "\n",
        "    # Step 5: Build deep GRU model\n",
        "    input_layer = Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
        "    x = GRU(128, return_sequences=True)(input_layer)\n",
        "    x = LayerNormalization()(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    x = GRU(64)(x)\n",
        "    x = LayerNormalization()(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    output = Dense(1)(x)\n",
        "\n",
        "    model = Model(inputs=input_layer, outputs=output)\n",
        "    model.compile(optimizer=Adam(learning_rate=0.0002), loss='mse')\n",
        "\n",
        "    # Step 6: Train with early stopping\n",
        "    early_stop = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
        "    model.fit(X_train, y_train, epochs=200, batch_size=16, validation_split=0.1,\n",
        "              callbacks=[early_stop], verbose=0)\n",
        "\n",
        "    # Step 7: Predict & evaluate\n",
        "    y_pred_log = model.predict(X_test).flatten()\n",
        "    y_pred = np.expm1(y_pred_log)\n",
        "    y_true = np.expm1(y_test)\n",
        "\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    mape = np.mean(np.abs((y_true - y_pred) / np.maximum(y_true, 1))) * 100\n",
        "\n",
        "    print(f\"ğŸ“Š {city_code.upper()} GRU Model Results:\")\n",
        "    print(f\"MAE: {mae:.2f}\")\n",
        "    print(f\"RMSE: {rmse:.2f}\")\n",
        "    print(f\"MAPE: {mape:.2f}%\")\n",
        "\n",
        "# Run both cities with optimal window sizes\n",
        "run_gru_model('sj', window=12)  # San Juan\n",
        "run_gru_model('iq', window=8)   # Iquitos\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bpcrVTF97zKW",
        "outputId": "1792f8fb-a355-4ea6-811a-7b8e7becaaf3"
      },
      "id": "bpcrVTF97zKW",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ™ï¸ Running GRU for city: SJ (window=12)\n",
            "\u001b[1m6/6\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
            "ğŸ“Š SJ GRU Model Results:\n",
            "MAE: 19.46\n",
            "RMSE: 34.80\n",
            "MAPE: 107.70%\n",
            "\n",
            "ğŸ™ï¸ Running GRU for city: IQ (window=8)\n",
            "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
            "ğŸ“Š IQ GRU Model Results:\n",
            "MAE: 7.48\n",
            "RMSE: 12.92\n",
            "MAPE: 121.22%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ğŸ§  LSTMæ·±åº¦å­¦ä¹ æ¨¡å‹ï¼šåˆ†åˆ«å»ºæ¨¡ San Juan (sj) å’Œ Iquitos (iq)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout, LayerNormalization\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow as tf\n",
        "\n",
        "def run_lstm_model(city_code, window):\n",
        "    print(f\"\\nğŸ™ï¸ Running LSTM for city: {city_code.upper()} (window={window})\")\n",
        "\n",
        "    # Step 1: Load and merge data\n",
        "    features_df = pd.read_csv('DengAI_train.csv')\n",
        "    labels_df = pd.read_csv('DengAI_Predicting_Disease_Spread_-_Training_Data_Labels.csv')\n",
        "    df = pd.merge(features_df, labels_df, on=['city', 'year', 'weekofyear'])\n",
        "\n",
        "    # Step 2: Filter city and clean\n",
        "    df = df[df['city'] == city_code].copy()\n",
        "    df.reset_index(drop=True, inplace=True)  # â† å…³é”®ä¿®å¤\n",
        "    df.drop(columns=['week_start_date', 'city'], inplace=True)\n",
        "    df = df.ffill()\n",
        "\n",
        "    # Step 3: Scale features\n",
        "    scaler = StandardScaler()\n",
        "    X = df.drop(columns=['total_cases'])\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "    y = np.log1p(df['total_cases'])\n",
        "\n",
        "    # Step 4: Create sequences\n",
        "    def create_sequences(X, y, window):\n",
        "        Xs, ys = [], []\n",
        "        for i in range(window, len(X)):\n",
        "            Xs.append(X[i-window:i])\n",
        "            ys.append(y[i])\n",
        "        return np.array(Xs), np.array(ys)\n",
        "\n",
        "    X_seq, y_seq = create_sequences(X_scaled, y, window=window)\n",
        "    split = int(0.8 * len(X_seq))\n",
        "    X_train, X_test = X_seq[:split], X_seq[split:]\n",
        "    y_train, y_test = y_seq[:split], y_seq[split:]\n",
        "\n",
        "    if len(X_train) == 0 or len(X_test) == 0:\n",
        "        print(\"âŒ Not enough data to train LSTM for this city.\")\n",
        "        return\n",
        "\n",
        "    # Step 5: Build LSTM model\n",
        "    input_layer = Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
        "    x = LSTM(128, return_sequences=True)(input_layer)\n",
        "    x = LayerNormalization()(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    x = LSTM(128)(x)\n",
        "    x = LayerNormalization()(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    output = Dense(1)(x)\n",
        "\n",
        "    model = Model(inputs=input_layer, outputs=output)\n",
        "    model.compile(optimizer=Adam(learning_rate=0.0003), loss='mse')\n",
        "\n",
        "    # Step 6: Train\n",
        "    early_stop = EarlyStopping(monitor='val_loss', patience=12, restore_best_weights=True)\n",
        "    model.fit(X_train, y_train, epochs=150, batch_size=16, validation_split=0.1,\n",
        "              callbacks=[early_stop], verbose=0)\n",
        "\n",
        "    # Step 7: Evaluate\n",
        "    y_pred_log = model.predict(X_test).flatten()\n",
        "    y_pred = np.expm1(y_pred_log)\n",
        "    y_true = np.expm1(y_test)\n",
        "\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    mape = np.mean(np.abs((y_true - y_pred) / np.maximum(y_true, 1))) * 100\n",
        "\n",
        "    print(f\"ğŸ“Š {city_code.upper()} Model Results:\")\n",
        "    print(f\"MAE: {mae:.2f}\")\n",
        "    print(f\"RMSE: {rmse:.2f}\")\n",
        "    print(f\"MAPE: {mape:.2f}%\")\n",
        "\n",
        "# åˆ†åˆ«è¿è¡Œ San Juanï¼ˆ12å‘¨çª—å£ï¼‰å’Œ Iquitosï¼ˆ6å‘¨çª—å£ï¼‰\n",
        "run_lstm_model('sj', window=12)\n",
        "run_lstm_model('iq', window=6)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ceTJo4Sd78M3",
        "outputId": "48971fd4-0b2d-494b-8921-b233df595663"
      },
      "id": "ceTJo4Sd78M3",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ™ï¸ Running LSTM for city: SJ (window=12)\n",
            "\u001b[1m6/6\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
            "ğŸ“Š SJ Model Results:\n",
            "MAE: 17.37\n",
            "RMSE: 31.29\n",
            "MAPE: 88.72%\n",
            "\n",
            "ğŸ™ï¸ Running LSTM for city: IQ (window=6)\n",
            "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
            "ğŸ“Š IQ Model Results:\n",
            "MAE: 7.85\n",
            "RMSE: 11.98\n",
            "MAPE: 236.03%\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:base] *",
      "language": "python",
      "name": "conda-base-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}